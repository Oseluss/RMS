{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms.train as train\n",
    "import algorithms.utils as utils\n",
    "import algorithms.models as models\n",
    "from enviroments import env_red, env_red_p2p, env_red_toy1, env_hubs1\n",
    "from algorithms.agents import reinforce\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_hubs1(model=\"Exp\", T=2000)\n",
    "M = 33\n",
    "F = 2\n",
    "\n",
    "#Dimensiónes del espacio de estados\n",
    "dims_state = list(env.C + 1)\n",
    "dims_state.append(env.T+1)\n",
    "\n",
    "#Dimensiones del espacio de acciones\n",
    "dims_action = [M, F]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = models.PolicyNetwork(\n",
    "    num_inputs=len(dims_state), \n",
    "    num_hiddens=[64, 64], \n",
    "    num_outputs=[M,F], \n",
    "    model=\"Softmax\"\n",
    ").double()\n",
    "\n",
    "critc = models.ValueNetwork(\n",
    "    num_inputs=len(dims_state), \n",
    "    num_hiddens=[64, 64], \n",
    "    num_outputs=1, \n",
    ").double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = reinforce.ReinforceSoftmaxNN(actor, critc, gamma=.99, tau=.99, lr_actor= 1e-6)\n",
    "Trainer = train.Trainer(\"sgd\", \"sgd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0416, -0.0245],\n",
      "        [ 0.0251,  0.0199],\n",
      "        [ 0.0195,  0.0334],\n",
      "        [ 0.0049,  0.0310],\n",
      "        [-0.0631, -0.0165],\n",
      "        [ 0.0368,  0.0637],\n",
      "        [-0.0230,  0.0300],\n",
      "        [-0.0375, -0.0238],\n",
      "        [-0.0500, -0.0018],\n",
      "        [ 0.0224,  0.0063],\n",
      "        [-0.0225, -0.0110],\n",
      "        [ 0.0154,  0.0033],\n",
      "        [ 0.0394,  0.0063],\n",
      "        [ 0.0040,  0.0156],\n",
      "        [ 0.0274,  0.0211],\n",
      "        [-0.0217, -0.0420],\n",
      "        [ 0.0102, -0.0187],\n",
      "        [-0.0078,  0.0215],\n",
      "        [-0.0026, -0.0167],\n",
      "        [-0.0149, -0.0484],\n",
      "        [-0.0292, -0.0062],\n",
      "        [ 0.0142,  0.0390],\n",
      "        [ 0.0072,  0.0502],\n",
      "        [-0.0019, -0.0319],\n",
      "        [-0.0243, -0.0211],\n",
      "        [ 0.0488,  0.0260],\n",
      "        [ 0.0152,  0.0035],\n",
      "        [ 0.0214, -0.0477],\n",
      "        [ 0.0107, -0.0338],\n",
      "        [-0.0332, -0.0161],\n",
      "        [ 0.0030, -0.0383],\n",
      "        [-0.0111, -0.0336],\n",
      "        [-0.0150,  0.0315]], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n",
      "Categorical(logits: torch.Size([33, 2]))\n",
      "La acción es: tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 0]), de longitud 33\n"
     ]
    }
   ],
   "source": [
    "s,_ = env.reset()\n",
    "\n",
    "state = torch.as_tensor(s).double()\n",
    "logits = actor(state).squeeze()\n",
    "print(logits)\n",
    "pi = torch.distributions.categorical.Categorical(logits=logits)\n",
    "print(pi)\n",
    "action = pi.sample()\n",
    "print(f\"La acción es: {action}, de longitud {len(action)}\")\n",
    "\n",
    "s,_ = env.reset()\n",
    "action = agent.policy.act(s)\n",
    "print(action[0])\n",
    "print(action[1])\n",
    "state = torch.as_tensor(s).double()\n",
    "print(agent.policy.evaluate_logprob(state, action[0]))\n",
    "print(agent.policy.evaluate_value(state))\n",
    "\n",
    "s,_ = env.reset()\n",
    "agent.select_action(s)\n",
    "print(agent.buffer.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 10])\n",
      "torch.Size([4000, 33])\n",
      "torch.Size([4000, 33])\n",
      "torch.Size([4000])\n"
     ]
    }
   ],
   "source": [
    "states = torch.stack(agent.buffer.states, dim=0).detach()\n",
    "print(states.shape)\n",
    "actions = torch.stack(agent.buffer.actions, dim=0).detach().squeeze()\n",
    "print(actions.shape)\n",
    "logprobs = agent.policy.evaluate_logprob(states, actions)\n",
    "print(logprobs.shape)\n",
    "\n",
    "advantages = agent.update_critic()\n",
    "print(advantages.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/15000: 65900 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, totals,_ \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(totals)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÍndice de la iteración\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/train.py:53\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, env, agent, epochs, max_steps, update_freq, initial_offset)\u001b[0m\n\u001b[1;32m     50\u001b[0m cum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m update_freq \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m initial_offset:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/train.py:22\u001b[0m, in \u001b[0;36mTrainer._update\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     20\u001b[0m         agent\u001b[38;5;241m.\u001b[39mupdate_actor(advantages, i)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m agent\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/reinforce.py:109\u001b[0m, in \u001b[0;36mReinforceSoftmaxNN.update_actor\u001b[0;34m(self, advantages, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Stochastic Gradient Ascent\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 109\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_logprob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mprod(logprobs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    111\u001b[0m     loss_actor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39madvantages\u001b[38;5;241m*\u001b[39mlogprobs\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/agents.py:35\u001b[0m, in \u001b[0;36mSoftmaxAgent.evaluate_logprob\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_logprob\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: torch\u001b[38;5;241m.\u001b[39mTensor, action: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Actor\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     action_logprob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action_logprob\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/agents.py:30\u001b[0m, in \u001b[0;36mSoftmaxAgent.pi\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(logits)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Distribution\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m pi \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pi\n",
      "File \u001b[0;32m~/Code/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:60\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, totals,_ = Trainer.train(env, agent, epochs=15000, max_steps=300, update_freq=4000, initial_offset=0)\n",
    "\n",
    "\n",
    "plt.plot(totals)\n",
    "plt.xlabel('Índice de la iteración')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Recompensa media')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_actor = dims_state + dims_action\n",
    "dimensions_critic = dims_state\n",
    "\n",
    "freq = 5000\n",
    "\n",
    "lr_actor = 1e-5\n",
    "\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor = models.PolicyPARAFAC(dimensions_actor, k=k, model= \"SoftMax\", scale = 1)\n",
    "critc = models.ValuePARAFAC(dimensions_critic, k=k, scale = 1)\n",
    "\n",
    "\n",
    "\n",
    "agent = reinforce.ReinforceSoftmaxNN(actor, critc,discretizer_actor= None,discretizer_critic=None, gamma=.99, tau=.99, lr_actor=lr_actor)\n",
    "Trainer = train.Trainer(\"sgd\", \"sgd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, indices: np.ndarray) -> torch.Tensor:\n",
    "        prod = torch.ones(self.k, dtype=torch.double)\n",
    "        for i in range(len(indices)):\n",
    "            idx = indices[i]\n",
    "            factor = self.factors[i]\n",
    "            prod *= factor[idx, :]\n",
    "        if len(indices) < len(self.factors):\n",
    "            res = []\n",
    "            for cols in zip(\n",
    "                *[self.factors[-(a + 1)].t() for a in reversed(range(self.nA))]\n",
    "            ):\n",
    "                kr = cols[0]\n",
    "                for j in range(1, self.nA):\n",
    "                    kr = torch.kron(kr, cols[j])\n",
    "                res.append(kr)\n",
    "            factors_action = torch.stack(res, dim=1)\n",
    "            return torch.matmul(prod, factors_action.T)\n",
    "        return torch.sum(prod, dim=-1)\n",
    "tiene menú contextual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorejuela/Code/RMS/algorithms/models.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indices=torch.tensor(indices,dtype=torch.long).view(1, -1)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expect a 1D vector, but got shape [3, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m s, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/agents.py:50\u001b[0m, in \u001b[0;36mSoftmaxAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m---> 50\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     52\u001b[0m     action_logprob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/agents.py:27\u001b[0m, in \u001b[0;36mSoftmaxAgent.pi\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     25\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(indices)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Distribution\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/models.py:77\u001b[0m, in \u001b[0;36mPolicyPARAFAC.forward\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     75\u001b[0m     prod \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m factor[idx, :]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactors):\n\u001b[0;32m---> 77\u001b[0m     action_khatri \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcartesian_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(prod \u001b[38;5;241m*\u001b[39m action_khatri, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/venv/lib/python3.8/site-packages/torch/functional.py:1099\u001b[0m, in \u001b[0;36mcartesian_prod\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(cartesian_prod, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m-> 1099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcartesian_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expect a 1D vector, but got shape [3, 5]"
     ]
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "agent.policy.act(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorejuela/Code/RMS/algorithms/models.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indices=torch.tensor(indices,dtype=torch.long).view(1, -1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got ParameterList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, totals,_ \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(totals)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÍndice de la iteración\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/train.py:42\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, env, agent, epochs, max_steps, update_freq, initial_offset)\u001b[0m\n\u001b[1;32m     39\u001b[0m cum_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m---> 42\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     state_next, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m max_steps:\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/reinforce.py:40\u001b[0m, in \u001b[0;36mReinforceSoftmaxNN.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     39\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(state)\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m---> 40\u001b[0m     action, action_logprob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/agents.py:50\u001b[0m, in \u001b[0;36mSoftmaxAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m---> 50\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     52\u001b[0m     action_logprob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/agents/agents.py:27\u001b[0m, in \u001b[0;36mSoftmaxAgent.pi\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     25\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(indices)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Distribution\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/RMS/algorithms/models.py:77\u001b[0m, in \u001b[0;36mPolicyPARAFAC.forward\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     75\u001b[0m     prod \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m factor[idx, :]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactors):\n\u001b[0;32m---> 77\u001b[0m     action_khatri \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcartesian_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(prod \u001b[38;5;241m*\u001b[39m action_khatri, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/venv/lib/python3.8/site-packages/torch/functional.py:1099\u001b[0m, in \u001b[0;36mcartesian_prod\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(cartesian_prod, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m-> 1099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcartesian_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got ParameterList"
     ]
    }
   ],
   "source": [
    "\n",
    "_, totals,_ = Trainer.train(env, agent, epochs=1, max_steps=1, update_freq=freq, initial_offset=0)\n",
    "\n",
    "\n",
    "plt.plot(totals)\n",
    "plt.xlabel('Índice de la iteración')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('Recompensa media')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
